{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd37b21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Módulo: Otros Tópicos\n",
    "## Aprendizaje por Refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ebd6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducción "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac261af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"figures/rl-1.png\" width=\"1000\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edae279",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"figures/refuerzo-1.png\" width=\"900\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2685a6b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fundamentos del aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b8f81c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalidades\n",
    "Un agente aprende en un ambiente interactivo mediante ensayo y error usando retroalimentación de sus propias acciones y experiencias.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/rl-2.png\" width=\"700\"/>\n",
    "</center>\n",
    "\n",
    "En el aprendizaje por refuerzo se premia o se castiga al modelo dependiendo si sus acciones conectan bien las entradas (atributos) y las salidas (objetivos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed12a40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Elementos básicos\n",
    "- Ambiente: mundo físico en que opera el agente\n",
    "- Estado: situación actual del agente\n",
    "- Recompensa: retroalimentación del ambiente al agente\n",
    "- Política: método que conecta el estado del agente a acciones\n",
    "- Valor: recompensa futura que el agente recibiría al tomar determinada acción\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/pacman-1.gif\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef12718c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dada una serie $S_i$ de posibles estados del agente\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/pacman-4.png\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "y dadas las posibles acciones $a_i$ que este puede tomar para cada estado\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/pacman-5.png\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    ", la política $\\pi$ mapea la probabilidad de tomar determinada acción dado un estado con el objetivo de maximizar la recompensa futura\n",
    "\n",
    "\\begin{align}\n",
    "    \\pi (a,s) = P(a_t | s_t)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff4699",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"figures/refuerzo-2.png\" width=\"900\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb3636",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La política puede ser determinística (siempre seguir el mismo camino), o bien puede ser probabilística.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/refuerzo-3.png\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046498e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploración y beneficio\n",
    "Para formar la política correcta, el agente enfrenta el dilema de explorar nuevos estados mientras maximiza la recompensa total. Para balancear ambos, la estrategia puede involucrar sacrificios de corto plazo.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/refuerzo-4.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "El agente debe poder recoger suficiente información para poder tomar la mejor decisión futura."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a65e8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"figures/refuerzo-5.png\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2aab03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Con o sin modelo\n",
    "\n",
    "El modelo consiste en la simulación de la dinámica del ambiente, por lo que el agente aprende \"cómo\" es el ambiente para planear mejor sus acciones.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/refuerzo-6.png\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "Sin un modelo, el agente solo aprende por ensayo y error para mejorar su conocimiento del ambiente.\n",
    "\n",
    "Esto suele ser impracticable dado el desconocimiento de las probabilidades a priori y a lo computacionalmente costoso de almacenar todas estas transiciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44528275",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algunos algoritmos para aprendizaje por refuerzo\n",
    "- Q-learning\n",
    "- Diferencia temporal\n",
    "- Redes neuronales antagónicas (adversarial networks)\n",
    "- Estado-acción-recompensa-estado-acción (SARSA)\n",
    "- Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10524fec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d746e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalidades\n",
    "Es una técnica sin modelo que se basa en optimizar un llamado valor $Q$\n",
    "\n",
    "$Q$ representa la calidad en términos de qué tan útil es una acción en ganar alguna recompensa futura\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/refuerzo-7.png\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "La técnica aprende el valor de la política óptima independiente de las acciones del agente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b00da52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Tabla $Q$**: estructura para calcular la máxima recompensa esperada por la acción en cada estado. Inicialmente los valores son cero.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/robot-1.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "| E/A | Arriba | Abajo | Izquierda | Derecha |\n",
    "| --- | --- | --- | --- | --- |\n",
    "|   Inicio   | 0 | 0 | 0 | 0 |\n",
    "|   Inactivo   | 0 | 0 | 0 | 0 |\n",
    "|   Potenciador   | 0 | 0 | 0 | 0 |\n",
    "|   Mina   | 0 | 0 | 0 | 0 |\n",
    "|   Fin   | 0 | 0 | 0 | 0 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb825075",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Algoritmo**\n",
    "<center>\n",
    "    <img src=\"figures/q-learning-1.png\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abdba8b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exploración VS Beneficio**\n",
    "\n",
    "La tabla-Q se actualiza mediante exploración y/o beneficio según una probabilidad $\\epsilon$\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/q-learning-2.png\" width=\"700\"/>\n",
    "</center>\n",
    "\n",
    "&#10148; Si la iteración es de exploración, el agente elige una acción al azar.\n",
    "\n",
    "&#10148; Si la iteración es de beneficio, el agente elige la mejor acción basada en la tabla-Q actual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a659206",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Actualización de valores**: se lleva a cabo mediante el uso de la ecuación de Bellman: <br>\n",
    "\n",
    "\\begin{split}\n",
    "    Q_{new}(s,a) = Q(s,a) + \\alpha \\left\\{ R_{t} + \\gamma \\max Q(s_{t+1},a) - Q(s,a) \\right\\}\n",
    "\\end{split}\n",
    "\n",
    "$Q(s,a)$ : valor actual <br>\n",
    "$\\alpha$ : tasa de aprendizaje <br>\n",
    "$\\gamma$ : factor de descuento <br>\n",
    "$R_{t}$ : recompensa por tomar acción en el estado <br>\n",
    "$\\max Q(s_{t+1},a)$ : estimado del valor óptimo futuro  <br>\n",
    "\n",
    "En el ejemplo del Robot se podría definir una recompensa de +1 al llegar al potenciador, de -100 al tocar una mina y de +100 al llegar a la meta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9152a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Optimización**\n",
    "\n",
    "&#10148; Al principio del entrenamiento el agente toma principalmente acciones para \n",
    "explorar el ambiente ($\\epsilon$ alto)\n",
    "\n",
    "&#10148; Luego, el agente comienza más a beneficiarse del ambiente sabiendo las acciones que dan mayor recompensa ($\\epsilon$ bajo)\n",
    "\n",
    "&#10148; Finalmente la tabla-Q podría quedar como sigue:\n",
    "\n",
    "| E/A | Arriba | Abajo | Izquierda | Derecha |\n",
    "| --- | --- | --- | --- | --- |\n",
    "|   Inicio   | 0 | 0 | 0 | 1 |\n",
    "|   Inactivo   | 0 | 0 | 0 | 1 |\n",
    "|   Potenciador   | 0 | 20 | 0 | 0 |\n",
    "|   Mina   | 0 | 2 | 0 | 0 |\n",
    "|   Fin   | 0 | 5 | 0 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb93aee8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ejemplo implementación Q-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bbcc23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Caso de análisis\n",
    "En un tablero el agente debe pasar a buscar un objeto y luego dejarlo en un punto determinado.\n",
    "\n",
    "Hay 6 posibles acciones que el agente puede realizar.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/ex-q-learning-1.png\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a792cf0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Se definen las siguientes recompensas según los posibles estados:\n",
    "- Salirse del tablero -10\n",
    "- Moverse en el tablero -1\n",
    "- Recoger el objeto erróneamente -10\n",
    "- Recoger el objeto exitósamente +20\n",
    "- Dejar el objeto erróneamente -10\n",
    "- Dejar el objeto exitósamente +20\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/ex-q-learning-1.png\" width=\"500\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd597bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Field:\n",
    "    def __init__(self, size, item_pickup, item_drop_off, start_position):\n",
    "        self.size = size\n",
    "        self.item_pickup = item_pickup\n",
    "        self.item_drop_off = item_drop_off\n",
    "        self.position = start_position\n",
    "        self.item_in_car = False\n",
    "        \n",
    "    def get_number_of_states(self):\n",
    "        return self.size*self.size*self.size*self.size*2\n",
    "    \n",
    "    def get_state(self):\n",
    "        state = self.position[0]*self.size*self.size*self.size*2\n",
    "        state = state + self.position[1]*self.size*self.size*2\n",
    "        state = state + self.item_pickup[0]*self.size*2\n",
    "        state = state + self.item_pickup[1]*2\n",
    "        if self.item_in_car:\n",
    "            state = state + 1\n",
    "        return state\n",
    "        \n",
    "    def make_action(self, action):\n",
    "        (x, y) = self.position\n",
    "        if action == 0:  # Go South\n",
    "            if y == self.size - 1:\n",
    "                return -10, False\n",
    "            else:\n",
    "                self.position = (x, y + 1)\n",
    "                return -1, False\n",
    "        elif action == 1:  # Go North\n",
    "            if y == 0:\n",
    "                return -10, False\n",
    "            else:\n",
    "                self.position = (x, y - 1)\n",
    "                return -1, False\n",
    "        elif action == 2:  # Go East\n",
    "            if x == 0:\n",
    "                return -10, False\n",
    "            else:\n",
    "                self.position = (x - 1, y)\n",
    "                return -1, False\n",
    "        elif action == 3:  # Go West\n",
    "            if x == self.size - 1:\n",
    "                return -10, False\n",
    "            else:\n",
    "                self.position = (x + 1, y)\n",
    "                return -1, False\n",
    "        elif action == 4:  # Pickup item\n",
    "            if self.item_in_car:\n",
    "                return -10, False\n",
    "            elif self.item_pickup != (x, y):\n",
    "                return -10, False\n",
    "            else:\n",
    "                self.item_in_car = True\n",
    "                return 20, False\n",
    "        elif action == 5:  # Drop off item\n",
    "            if not self.item_in_car:\n",
    "                return -10, False\n",
    "            elif self.item_drop_off != (x, y):\n",
    "                self.item_pickup = (x, y)\n",
    "                self.item_in_car = False\n",
    "                return -10, False\n",
    "            else:\n",
    "                return 20, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7830ae7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def naive_solution():\n",
    "    size = 10\n",
    "    item_start = (0, 0)\n",
    "    item_drop_off = (9, 9)\n",
    "    start_position = (0, 9)\n",
    "    \n",
    "    field = Field(size, item_start, item_drop_off, start_position)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = random.randint(0, 5)\n",
    "        reward, done = field.make_action(action)\n",
    "        steps = steps + 1\n",
    "    \n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae91adf4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35615\n"
     ]
    }
   ],
   "source": [
    "print( naive_solution() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18959ab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145056.63\n"
     ]
    }
   ],
   "source": [
    "runs = [naive_solution() for _ in range(100)]\n",
    "print(sum(runs)/len(runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d20ca21a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "size = 10\n",
    "item_start = (0, 0)\n",
    "item_drop_off = (9, 9)\n",
    "start_position = (0, 9)\n",
    "field = Field(size, item_start, item_drop_off, start_position)\n",
    "number_of_states = field.get_number_of_states()\n",
    "number_of_actions = 6\n",
    "q_table = np.zeros((number_of_states, number_of_actions))\n",
    "epsilon = 0.1\n",
    "alpha = 0.4\n",
    "gamma = 0.9\n",
    "for _ in range(1000):\n",
    "    field = Field(size, item_start, item_drop_off, start_position)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        state = field.get_state()\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.randint(0, 5)\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "            \n",
    "        reward, done = field.make_action(action)\n",
    "        \n",
    "        new_state = field.get_state()\n",
    "        new_state_max = np.max(q_table[new_state])\n",
    "        \n",
    "        q_table[state, action] = q_table[state, action] + alpha*(reward + gamma*new_state_max - q_table[state, action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54908aaf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def reinforcement_learning():\n",
    "    epsilon = 0.1\n",
    "    alpha = 0.4\n",
    "    gamma = 0.9\n",
    "    \n",
    "    field = Field(size, item_start, item_drop_off, start_position)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        state = field.get_state()\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.randint(0, 5)\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "            \n",
    "        reward, done = field.make_action(action)\n",
    "        \n",
    "        new_state = field.get_state()\n",
    "        new_state_max = np.max(q_table[new_state])\n",
    "        \n",
    "        q_table[state, action] = q_table[state, action] + alpha*(reward + gamma*new_state_max - q_table[state, action])\n",
    "        steps = steps + 1\n",
    "    \n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5694f14a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954\n"
     ]
    }
   ],
   "source": [
    "print( reinforcement_learning() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e49c7bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2218.65\n"
     ]
    }
   ],
   "source": [
    "runs_rl = [reinforcement_learning() for _ in range(100)]\n",
    "print(sum(runs_rl)/len(runs_rl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf28f29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sumario\n",
    "- En el aprendizaje por refuerzo existe interacción entre un agente y el ambiente.\n",
    "- El agente aprende las mejores acciones a tomar en base a prueba y error o en base a un modelo que aprende.\n",
    "- El algoritmo de Q-learning se basa en aprender una tabla con las recompensas a obtener para cada estado y acción."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
