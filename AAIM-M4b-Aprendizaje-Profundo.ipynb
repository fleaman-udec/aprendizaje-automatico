{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7da7ffe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Módulo: Otros Tópicos\n",
    "## Aprendizaje Profundo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc50d5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducción\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b8664d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"figures/intro-deep-1.jpg\" width=\"1300\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fcda81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalidades\n",
    "\n",
    "Una ANN realiza **composiciones de funciones** simples para formar una función más compleja.\n",
    "\n",
    "En teoría, una única composición de funciones simples puede aproximar a casi cualquier función compleja, pero esto requiere una **enorme cantidad de parámetros**.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/ann-ex-1.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "Incluso es posible demostrar que un MLP con varias capas ocultas usando la **función identidad**, no tiene ninguna ventaja frente al MLP con solo una capa oculta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00688e6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El aprendizaje profundo realiza **repetidas composiciones** de funciones no-lineales (muchas capas ocultas), lo que tiene un gran poder expresivo\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{\\mathbf{Y}} = f_1( f_2( f_3( ... f_n( \\mathbf{X} ) ) ) )\n",
    "\\end{align}\n",
    "\n",
    "Esto puede reducir considerablemente la cantidad de parámetros totales necesarios dependiendo de las funciones de activación usadas.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/intro-deep-2.jpg\" width=\"600\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3709dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las arquitecturas profundas aprovechan mejor los **patrones repetitivos** en los datos para lograr mejor generalización incluso en áreas del espacio con pocos o sin datos.\n",
    "\n",
    "Usualmente estos patrones repetitivos se aprenden como pesos de **atributos jerarquizados**.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/intro-deep-3.gif\" width=\"800\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27dc39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo\n",
    "Una función unidimensional toma alternadamente valores +1 o -1 un total de 8 veces. ¿De qué forma puede construirse una ANN para aproximar esta función?\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/funscale-deep-1.png\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b4a8f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usando una **ANN superficial**, deberían por lo menos haber 8 unidades para aproximar cada valor (sin contar las unidades de sesgo u offset)\n",
    "\n",
    "Así, dependiendo de la entrada, debería \"activarse\" una de las 8 unidades para entregar la salida correcta\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/ann-ex-3.png\" width=\"1000\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b01be32",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usando una **ANN profunda**, se podrían usar 3 capas de 2 unidades para obtener un total de 8 \"caminos\" que se activan para entregar la salida correcta dependiendo de la entrada.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/funscale-deep-3.png\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "Así, la ANN profunda aprende parámetros de forma **jerarquizada**. \n",
    "\n",
    "Por ejemplo, la primera capa aprende un escalón simple, la segunda capa aprende un escalón doble, y sucesivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6f7628",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.611934613526046e-10\n",
      "[ 0.99998867 -0.99999556  0.99996998 -1.00001221  0.99999292 -0.99999502\n",
      "  0.99998095 -0.9999787 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = [[0.], [1.], [2.], [3.], [4.], [5.], [6.], [7.]]\n",
    "y = [1., -1., 1., -1., 1., -1., 1., -1.]\n",
    "\n",
    "clf = MLPRegressor(random_state=4, tol=1.e-8, alpha=1.e-8, hidden_layer_sizes=(512,),\n",
    "                   activation='tanh', solver='lbfgs', max_iter=int(1e8), batch_size=1)\n",
    "clf.fit(X, y)\n",
    "y_predict = clf.predict(X)\n",
    "print(mean_squared_error(y, y_predict))\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73814e70",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3175349582550622e-09\n",
      "[ 1.00005362 -0.9999863   0.99999946 -1.00006921  1.0000571  -1.00002413\n",
      "  1.00006667 -1.00004899]\n"
     ]
    }
   ],
   "source": [
    "clf = MLPRegressor(random_state=4, tol=1.e-8, alpha=1.e-8, hidden_layer_sizes=(16, 16, 16, 16, 16, 16, 16, 16),\n",
    "                   activation='tanh', solver='lbfgs', max_iter=int(1e8), batch_size=1)\n",
    "clf.fit(X, y)\n",
    "y_predict = clf.predict(X)\n",
    "print(mean_squared_error(y, y_predict))\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c6f6a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Arquitecturas comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb31b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Redes superficiales (shallow network)\n",
    "La mayoría de los modelos de aprendizaje automático (regresión lineal, logística, SVM, PCA, etc.) pueden simularse como una red neuronal de 1 ó 2 capas ocultas.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/ann-5.png\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "Un tipo de red superficial sería el perceptrón multicapa con 1 ó 2 capas ocultas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081c1f2f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Estas redes están totalmente conectadas, es decir, hay conexión directa entre todas las unidades.\n",
    "\n",
    "Son un tipo de red prealimentada (feed forward) en las que el flujo va direccionalmente desde capas anteriores a capas posteriores.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/ann-gif-1.gif\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049af7a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Redes de base radial (RBF, radial basis function)\n",
    "\n",
    "A pesar de no ser profunda, difiere de las redes prealimentadas ya que tienen una parte entrenada no supervisadamente y una parte entrenada supervisadamente.\n",
    "\n",
    "Comúnmente tienen una capa oculta con una cantidad de unidades superior al de la capa de entrada.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/rbf-1.png\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6def74",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Primero, de manera no supervisada se obtiene el **vector prototipo** $\\bar{\\mu_i}$ para la $i$-ésima unidad oculta. Además, se obtiene un **ancho de banda** $\\sigma_i$ para cada unidad oculta.\n",
    "\n",
    "Luego, para cada vector de atributos $\\bar{X}$ que pasa a la capa oculta se define la **función de activación radial** $\\phi_{i}(\\bar{X})$ como sigue:\n",
    "\n",
    "\\begin{split}\n",
    "    h_{i} = \\phi_{i}(\\bar{X}) = \\text{exp} \\left( {- \\frac{ \\| \\bar{X} - \\bar{\\mu_i} \\|^{2} }{ 2 \\sigma_{i}^2 } } \\right) \\; \\forall i \\in \\{1, ..., m\\}\n",
    "\\end{split}\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/rbf-2.jpeg\" width=\"500\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d89661",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cada una de las $m$ unidades ocultas tendrá gran influencia en los datos cercanos a su vector prototipo. Es decir, este tipo de red actúa como un método de **agrupamiento**.\n",
    "\n",
    "Luego se calcula la predicción en la capa de salida de la red como se fuera un **perceptrón**, es decir:\n",
    "\n",
    "\\begin{split}\n",
    "    \\hat{y} = \\sum_{i=1}^{m} w_{i} \\phi_{i}(\\bar{X}) = \\sum_{i=1}^{m} w_{i} \\text{exp} \\left( {- \\frac{ \\| \\bar{X} - \\bar{\\mu_i} \\|^{2} }{ 2 \\sigma_{i}^2 } } \\right)\n",
    "\\end{split}\n",
    "\n",
    "Los valores de los pesos $w_{i}$ se aprenden de forma **supervisada** como en una red prealimentada. También se implementa una neurona de sesgo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5871c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Máquinas restringidas de Boltzmann\n",
    "\n",
    "Utilizan el concepto de minimización de energía para crear una red neuronal de forma no supervisada.\n",
    "\n",
    "También se puede ser un entrenamiento posterior de forma supervisada.\n",
    "\n",
    "Es diferente a las redes prealimentadas en varios sentidos:\n",
    "- Modela la **probabilidad conjunta** de los atributos en vez de minimizar una función de costo.\n",
    "- Tienen **flujo no directo**, ya que aprenden relaciones probabilísticas en vez de mapeos entrada-salida.\n",
    "- Crea representaciones **latentes** (ocultas) de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c337f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Redes neuronales recurrentes\n",
    "\n",
    "Las conexiones entre unidades pueden crear **ciclos**, por lo que la salida de una neurona puede afectar a su propia entrada.\n",
    "\n",
    "Son especialmente útiles para modelar comportamiento **temporales** o dinámicos.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/recurrent-1.png\" width=\"500\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159df964",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Redes neuronales convolucionales \n",
    "(CNN, convolutional neural networks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a986e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamentos\n",
    "\n",
    "Históricamente han sido el tipo de ANN más exitosa, usadas principalmente para reconocimiento de imágenes, localización de objetos y procesamiento de texto, entre otros.\n",
    "\n",
    "Están inspiradas por el funcionamiento del cortex visual de los gatos, en donde porciones específicas del campo visual activan ciertas neuronas.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/cnn-0.png\" width=\"1000\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4da78",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Están basadas en el concepto de la convolución entre dos funciones:\n",
    "\n",
    "\\begin{align}\n",
    "    (f \\ast g)(t) = \\int_{-\\infty}^\\infty f(\\tau) g(t - \\tau) d\\tau\n",
    "\\end{align}\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/convo-gif-1.gif\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/convo-gif-3.gif\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bdb75b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Capas convolucionales\n",
    "\n",
    "Intuitivamente la operación de convolución utiliza filtros (kernels) adecuados para detectar determinados patrones dentro de las imágenes.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/convolution-2-gif.gif\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "Sin embargo, algunos filtros son más abstractos y no tienen necesariamente una interpretación intuitiva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef4214",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dada una capa $i$, los valores de la capa $i+1$ son el producto punto entre los valores (pesos) del filtro y regiones espaciales de la capa actual de igual dimensión\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/convolution-gif.gif\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "Al resultado de la operación también se le puede aplicar una función de activación (e.g. ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60165bff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Al aplicar más de un filtro a una misma capa, se pueden generar capas sucesivas tridimensionales.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/convo-4.gif\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "*Ej.: al aplicar 3 filtros de profundidad 1 a una capa de profundidad 1, se generará una capa de profundidad 3*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c0e12e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "También existen filtros tridimensionales de profundidad determinada para aplicar a capas que ya son tridimensionales.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/convo-5.gif\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "*Ej.: al aplicar 1 filtro de profundidad 3 a una capa de profundidad 3, se generará una capa de profundidad 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e22b5c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En general, es posible aplicar cualquier cantidad de filtros de profundidad determinada a cualquier capa de cualquier otra profundidad.\n",
    "\n",
    "<br><center>\n",
    "    <img src=\"figures/convo-6.png\" width=\"1000\"/>\n",
    "</center>\n",
    "\n",
    "*Ej.: aplicar un filtro de 3x3x3 sobre una capa de 4x5x4 resulta en una capa de 2x3x2*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05ade2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Capas de submuestreo (pooling)\n",
    "\n",
    "El submuestreo consiste en reducir la dimensionalidad de las capas convolucionales.\n",
    "\n",
    "Se busca reducir el costo computacional, aumentar la tolerancia a pequeñas distorsiones y hacer la CNN lo más genérica posible.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/pooling-2.jpeg\" width=\"500\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c6c11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los tipos más comunes son el submuestreo promediado y submuestreo máximo.\n",
    "- Average pooling: se van tomando promedios cada $a x b$ posiciones\n",
    "- Max pooling: se van tomando el valor máximo cada $a x b$ posiciones\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/pooling-gif.gif\" width=\"700\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf6306",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Capa totalmente conectada (fully connected)\n",
    "\n",
    "Luego de sucesivas capas convolucionales y de submuestreo se suele aplicar una capa totalmente conectada.\n",
    "\n",
    "Esto se hace para ponderar toda la información presente en la capa anterior, que hasta el momento retenía la estructura espacial de la entrada original.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/convo-9.png\" width=\"1100\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee1d39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Capa softmax\n",
    "\n",
    "Si la CNN se utiliza para clasificación, luego de la capa totalmente conectada se aplica una capa softmax para predecir las probabilidades para cada clase.\n",
    "\n",
    "<br><center>\n",
    "    <img src=\"figures/softmax.png\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2716a385",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Capa de dilución (dropout)\n",
    "\n",
    "Se trata de una capa especial usada durante el entrenamiento para prevenir el sobreajuste.\n",
    "\n",
    "Consiste en aleatoriamente no considerar algunas unidades durante el entrenamiento.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/dropout-1.png\" width=\"700\"/>\n",
    "</center>\n",
    "\n",
    "Con esta técnica se busca aumentar la capacidad de generalización de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eade3d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Funcionamiento de una CNN\n",
    "\n",
    "Las conexiones en una CNN son muy escasas, ya que una activación en una capa particular es función solo de una pequeña región espacial de la capa anterior.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/cnn-1.jpeg\" width=\"1200\"/>\n",
    "</center>\n",
    "\n",
    "Los atributos en las capas inferiores capturan formas primitivas como líneas, mientras que los atributos es capas superiores captan formas complejas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee9480",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Por su funcionamiento jerarquico en encontrar atributos de bajo nivel y combinarlos para formar atributos de alto nivel, es posible utilizar CNN preentrenadas para distintos tipos de aplicaciones.\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/cnn-2.png\" width=\"1200\"/>\n",
    "</center>\n",
    "\n",
    "En estos casos solo se realiza un ajuste fino en la última capa de la CNN para adaptarla al caso en particular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b824c01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arquitectura LeNet-5\n",
    "\n",
    "Es una de las primeras arquitecturas de CNN preentrenadas para **reconocer texto**.\n",
    "\n",
    "Consta de **3 capas convolucionales** combinadas son **submuestreo promedio**. Luego siguen capas **totalmente conectadas** y un clasificador **softmax**.\n",
    "\n",
    "La entrada es una imagen de 32x32 pixeles en escala de grises, por lo que el número de canales (o profundidad) es 1.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/lenet-1.png\" width=\"200\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4afbb0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La primera convolución utiliza **6 filtros** de tamaño **5x5**.\n",
    "\n",
    "Como resultado se obtiene un mapa de atributos de tamaño **28x28x6**.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/lenet-2.png\" width=\"900\"/>\n",
    "</center>\n",
    "\n",
    "Aquí el número de canales es igual a la cantidad de filtros aplicados, osea **6**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f9391f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La primera operación de **submuestreo** (pooling) promedia los valores en regiones de **2x2**.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/lenet-3.png\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "Como resultado, se reduce el tamaño de los mapas de atributos a la mitad, pero el número de canales se mantiene igual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abad9ed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Luego se vuelve a realizar otra convolución, otro submuestreo y otra convolución hasta llegar a un mapa de atributos lineal de 120 valores.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/lenet-4.png\" width=\"1000\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad898d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finalmente se agrega una capa **totalmente conectada** con **84** unidades y una capa de clasificación **softmax** con **10** unidades.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/lenet-5.png\" width=\"1000\"/>\n",
    "</center>\n",
    "\n",
    "Sin considerar la capa softmax, todas las demás capas utilizan la función de activación **tanh**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982921ae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"figures/lenet-gif.gif\" width=\"1200\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c110161a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementación en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50182f54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Librerías\n",
    "\n",
    "Hay principalmente 3 librerías para implementar modelos de aprendizaje profundo en Python:\n",
    "\n",
    "- Tensorflow\n",
    "- Pytorch\n",
    "- Keras\n",
    "\n",
    "Estas librerías permiten formar fácilmente capas convolucionales, pooling, etc.\n",
    "\n",
    "Se diferencian en cuanto a la complejidad de usar, compatibilidad, velocidad, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45bb17c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"figures/comparison-deep.png\" width=\"1200\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500cfd6f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uso de la GPU\n",
    "\n",
    "Debido a la considerable mayor cantidad de atributos y operaciones computacionales presentes en una red profunda, los tiempos de entrenamiento son mucho mayores.\n",
    "\n",
    "Esto se logra solucionar mediante el uso de la tarjeta gráfica (GPU) en vez del procesador central (CPU) para realizar los cálculos.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/gpu-1.png\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea068d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El poder usar la GPU para los cálculos se logra mediante la plataforma **CUDA** de Nvidia.\n",
    "\n",
    "Las librerías anteriores pueden detectar si la GPU del PC es compatible con CUDA. \n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/cuda.jpg\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "En **Keras**, si la GPU es compatible los cálculos se realizarán automáticamente en ella.\n",
    "\n",
    "Aquí se puede revisar el listado de GPUs compatibles:\n",
    "\n",
    "https://developer.nvidia.com/cuda-gpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f958f30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNN para reconocimiento de dígitos escritos a mano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb6c307",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17488687830371813315\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8649b111",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n",
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "# parametros de los datos\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# carga los datos y crea sets de entrenamiento y test\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "450ab49f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la clase verdadera es:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valor máximo en la imagen =  255\n"
     ]
    }
   ],
   "source": [
    "idx = 7\n",
    "print('la clase verdadera es: ', y_train[idx])\n",
    "plt.imshow(x_train[idx], cmap='Greys')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "print('valor máximo en la imagen = ', np.max(x_train[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f1d0ede",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "x_test shape: (10000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# escala imagenes al rango 0-1 \n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# asegurarse que las imagenes tengan tamaño 28x28x1\n",
    "x_train = np.expand_dims(x_train, -1) #agrega dimension extra\n",
    "x_test = np.expand_dims(x_test, -1) #agrega dimension extra\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c877f32",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(60000, 10)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# transforma la variable objetivo en un vector de clases\n",
    "print(y_train[idx])\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(y_train.shape)\n",
    "print(y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e02de3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                16010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(), layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af4c210",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 26s 59ms/step - loss: 0.3655 - accuracy: 0.8897 - val_loss: 0.0817 - val_accuracy: 0.9792\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 25s 58ms/step - loss: 0.1135 - accuracy: 0.9657 - val_loss: 0.0591 - val_accuracy: 0.9828\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 26s 60ms/step - loss: 0.0845 - accuracy: 0.9741 - val_loss: 0.0516 - val_accuracy: 0.9853\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 26s 61ms/step - loss: 0.0718 - accuracy: 0.9776 - val_loss: 0.0420 - val_accuracy: 0.9873\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 26s 61ms/step - loss: 0.0633 - accuracy: 0.9810 - val_loss: 0.0407 - val_accuracy: 0.9888\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 25s 59ms/step - loss: 0.0575 - accuracy: 0.9822 - val_loss: 0.0396 - val_accuracy: 0.9880\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 26s 62ms/step - loss: 0.0523 - accuracy: 0.9841 - val_loss: 0.0374 - val_accuracy: 0.9907\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 26s 61ms/step - loss: 0.0482 - accuracy: 0.9849 - val_loss: 0.0372 - val_accuracy: 0.9885\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 25s 60ms/step - loss: 0.0445 - accuracy: 0.9860 - val_loss: 0.0347 - val_accuracy: 0.9907\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 27s 63ms/step - loss: 0.0420 - accuracy: 0.9869 - val_loss: 0.0317 - val_accuracy: 0.9920\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 26s 61ms/step - loss: 0.0420 - accuracy: 0.9870 - val_loss: 0.0317 - val_accuracy: 0.9922\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 25s 60ms/step - loss: 0.0392 - accuracy: 0.9876 - val_loss: 0.0307 - val_accuracy: 0.9922\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 26s 61ms/step - loss: 0.0367 - accuracy: 0.9885 - val_loss: 0.0311 - val_accuracy: 0.9918\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 26s 61ms/step - loss: 0.0349 - accuracy: 0.9886 - val_loss: 0.0287 - val_accuracy: 0.9923\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 26s 62ms/step - loss: 0.0333 - accuracy: 0.9888 - val_loss: 0.0291 - val_accuracy: 0.9923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ba4beba760>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "#compila el modelo, definiendo solver, metrica y funcion de costo\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "#comienza el entrenamiento\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d378f727",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.024575844407081604\n",
      "Test accuracy: 0.9919000267982483\n"
     ]
    }
   ],
   "source": [
    "#evalua el modelo para datos de test\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda91de3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**la clase verdadera es:  8\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "**la clase predicha es:  8\n"
     ]
    }
   ],
   "source": [
    "#verifica predicciones para algunos datos\n",
    "idx = 257 #259\n",
    "plt.imshow(x_test[idx], cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "print('**la clase verdadera es: ', np.argmax(y_test[idx]))\n",
    "\n",
    "prediction = model.predict(np.expand_dims(x_test[idx], 0))\n",
    "print('**la clase predicha es: ', np.argmax(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33660156",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Pero... ¿funciona realmente en la práctica?**\n",
    "\n",
    "<br><center>\n",
    "    <img src=\"figures/sus-1.png\" width=\"500\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32f5cdc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import PIL #modulo PILLOW para trabajar con imagenes \n",
    "\n",
    "im = PIL.Image.open(\"others/5.jpg\") #abre la imagen\n",
    "im.show() #muestra la imagen en nueva ventana\n",
    "im = im.resize((28, 28)) #redimensiona a 28x28 px\n",
    "im = im.convert(mode=\"L\") #pasa a escala de grises\n",
    "im.show() #muestra la imagen en nueva ventana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c7b6160",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "imarr = np.asarray(im) #pasa la imagen a array\n",
    "imarr = np.max(imarr) - imarr #invierte colores en el array\n",
    "imarr = imarr/np.max(imarr) #normaliza en el rango 0-1\n",
    "imarr = np.expand_dims(imarr, -1) #agrega la dimension de la profunidad al final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb0cb11d",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQfElEQVR4nO3db4id5ZnH8d9lTNTYiJnEuIMJaQ0KirJpGZNFpbjIlpg3MS9amhcxC7KpoNJCX6zoi/oqyLJt6IslkK6h6VJTCq0YUHYbYkUapGQSs5psWP+E2XTqkL/+q5r/176Yx3Ya57nv47nPc54Tr+8HhjNz7nnOc50z85vnnLnO/dzm7gLwxXdZ2wUA6A/CDgRB2IEgCDsQBGEHgri8nzubP3++L168uJ+7HAhm1nYJCGJsbEzHjx+f9heuKOxmtkLSjyXNkPTv7v5U6vsXL16sXbt2lewyVUuj25fc/mWXpZ9ANfnHoPR+tdmaze17kP+INll7atuRkZHasa6fxpvZDEn/Juk+SbdKWmNmt3Z7ewCaVfKafZmkt9z9kLufkfQLSat6UxaAXisJ+w2S/jDl6/Hqur9iZuvNbNTMRo8dO1awOwAlSsI+3QuHz7xQcffN7j7i7iPXXXddwe4AlCgJ+7ikRVO+XijpnbJyADSlJOy7Jd1kZl8xs1mSvi1pe2/KAtBrXbfe3P2cmT0i6b802Xrb4u4HMtsUtXJSLYfSVkdTdUnShQsXkuO51lzJ/nP3q7S11ubjWqLplmOTtXdbW1Gf3d1fkPRCyW0A6A/eLgsEQdiBIAg7EARhB4Ig7EAQhB0Ioq/z2aX2pkw22TctnUZa2odvs5fd5M+zzdtuc/psU7VxZAeCIOxAEIQdCIKwA0EQdiAIwg4EcUm13lLbttkqKZ0uWdqaS2mybXcpG+Sz6uZ0WxtHdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0I4pLqszd5uyV9+tIpqrntm6ztUlZyCu42Tz3eFo7sQBCEHQiCsANBEHYgCMIOBEHYgSAIOxBEX/vs7l7U921zznqJ0j56SU+46X5y6Wm0U2bMmJEcL3n/QtNLWTd5DoJuFYXdzMYkfSjpvKRz7j7Si6IA9F4vjux/7+7He3A7ABrEa3YgiNKwu6TfmNkeM1s/3TeY2XozGzWz0RMnThTuDkC3SsN+l7t/TdJ9kh42s69f/A3uvtndR9x9ZN68eYW7A9CtorC7+zvV5VFJz0pa1ouiAPRe12E3s6vNbM6nn0v6hqT9vSoMQG+V/Df+eknPVn3UyyU94+7/mduorT77F3EJ3k/NnDmzdizXqy51/vz55Hjqvp8+fTq57cTERHJ87969yfG77767dmx4eDi5bZvnASjZd+rx7jrs7n5I0t92uz2A/qL1BgRB2IEgCDsQBGEHgiDsQBB9n+J67ty52vGSqX1NTQvsRNOnsc6Nr1mzpnbsmmuuSW574403Jsfff//95PhHH32UHD916lTt2NmzZ5PbzpkzJzm+aNGi5Pjy5ctrx3LtrUE+VXS3rVqO7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQxEAt2ZybLlkyFbS0D5+qu+nTMedqf/HFF2vHNmzYkNz2lltuSY5feeWVyfG5c+d2PT579uzktrNmzUqO56bvpt7T0fQU1ianVHf7HgCO7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQRN/ns+d66SmpvmrudnN91VwvOzXe9Hz2Y8eOJccXLlxYO7Z27drktrledel9K9k+N9891UfPKe2Dl763og0c2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgiL7PZy/RVI++k9tO9elL58rnanv11VeT47fffnvX+z5z5kzX20rl57xvalt8Vva31My2mNlRM9s/5bohM9thZm9Wl+kzGABoXSeHpJ9KWnHRdY9J2unuN0naWX0NYIBlw+7uL0s6edHVqyRtrT7fKun+3pYFoNe6fbF5vbtPSFJ1uaDuG81svZmNmtnoiRMnutwdgFKN/zfe3Te7+4i7j8ybN6/p3QGo0W3Yj5jZsCRVl0d7VxKAJnQb9u2S1lWfr5P0XG/KAdCUbJ/dzLZJukfSfDMbl/QDSU9J+qWZPSjpsKRvdrrDknm+Jeduz819zm2f6oU3fQ7yXbt2JcfvvPPO2rHc+wdyj0vuPQRt9tmbvO3c/c79zJusLZWD1Fg27O6+pmbo3ty2AAYHb5cFgiDsQBCEHQiCsANBEHYgiIFasrnJFlaulZJrCZa0/Uqn1x44cCA5njtddImS0zVL6ce9zbZebt9NLh+e09RpqDmyA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQfV+yualeetPTTFNKe/jvvfdecnzHjh3J8RUrLj4f6F/s3r07ue3NN99cNH7VVVclx1PLLje5jHZO7mdS2uMfxNNgc2QHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSD6Pp+9rX54k/stve0rrrgiOb5hw4bk+NjYWO3Y3LnpBXZfeuml5Pgbb7yRHH/ooYeS46tWraody82VLz0PQKrXXdrDL+2jt7GUNUd2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQhioM4b3+Yc4Nz85pJeeunc6dx54VPbl+77yJEjyfEHHnggOX748OHasUcffTS5bWouvFT2+5Lr4ZfOtc9Jbd/Uks3Zis1si5kdNbP9U6570sz+aGb7qo+VudsB0K5O/jz9VNJ0p0LZ6O5Lq48XelsWgF7Lht3dX5Z0sg+1AGhQyQuPR8zsteppfu0bsM1svZmNmtnoyZP8zQDa0m3YN0laImmppAlJP6z7Rnff7O4j7j4yNDTU5e4AlOoq7O5+xN3Pu/sFST+RtKy3ZQHota7CbmbDU75cLWl/3fcCGAzZPruZbZN0j6T5ZjYu6QeS7jGzpZJc0pik73S6w5KecJNyfdVUbbl51Tm5+126RnqJ3EuvZ555Jjm+fPny2rHVq1cnt12wYEFyvOQ9BLmfWZNz6aX071tT58PPht3d10xz9dNdVwOgFbxdFgiCsANBEHYgCMIOBEHYgSAGaopriZLWWSdSrZYmp8e27dSpU8nxa6+9Njm+ZMmS2rE9e/Ykt125Mj2ZMtf+Kmlh5dqdudZarnWX0tTvC0d2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQii7332lKZ75Sm5nm1q3232+HNK+r1S/r7lahsfH68dyy0nnbvtkn506ZLMuZ9pmz+zOhzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiCIvvbZ3T3ZG22yd1na626r7ty+mzZ79uzk+CuvvJIcP336dO3Ybbfdltw2t2Rzrlc+yH34NnBkB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgBuq88U3OKc8p6Yvm+sGlcvctNT5r1qzktrk++qFDh5Lja9ZMt8jvX2zatKl2bObMmcltc+duv/zy9K9vU/PCpbLlonNKl5Oukz2ym9kiM/utmR00swNm9t3q+iEz22Fmb1aX6TMRAGhVJ0/jz0n6vrvfIunvJD1sZrdKekzSTne/SdLO6msAAyobdnefcPe91ecfSjoo6QZJqyRtrb5tq6T7G6oRQA98rn/QmdmXJX1V0u8lXe/uE9LkHwRJC2q2WW9mo2Y2evLkycJyAXSr47Cb2Zck/UrS99z9g063c/fN7j7i7iNDQ0Pd1AigBzoKu5nN1GTQf+7uv66uPmJmw9X4sKSjzZQIoBeyrTeb7CE8Lemgu/9oytB2SeskPVVdPtdIhT3S5umcP/gg/UQotyxyrj2War29/fbbyW2ff/755Pi2bduS4xs3bkyO33vvvbVjufudazHlfqZNtmpzU2CbbM11q5M++12S1kp63cz2Vdc9rsmQ/9LMHpR0WNI3G6kQQE9kw+7uv5NU92eo/s82gIHC22WBIAg7EARhB4Ig7EAQhB0Iou9TXFP96pJTMpeerjmnpC/77rvvJse3bNmSHB8bG0uOf/zxx7VjuR79HXfckRzfuXNncnz+/PnJ8U8++aR2LNerzsn1qlPjTU5/bVq3PXqO7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQRN+XbC45lXRqvOlljVO3nzvl8fDwcHL8iSeeSI7n5n2n+tW5PnvudMy5+5arrUltzAnvVMl7CErn8dfhyA4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQfS9z37mzJl+7vLPSpY9zo3nts31qnNKeranT59Ojud+Hrl9l9SWe9xK57unti8553zTWluyGcAXA2EHgiDsQBCEHQiCsANBEHYgCMIOBNHJ+uyLJP1M0t9IuiBps7v/2MyelPRPko5V3/q4u7+Qu72m5p2X9tFLNN2TLam9tJed6+mW3H5pHz33/oVU7bm58JfyeeXrdPKmmnOSvu/ue81sjqQ9ZrajGtvo7v/aXHkAeqWT9dknJE1Un39oZgcl3dB0YQB663M9jzKzL0v6qqTfV1c9YmavmdkWM5tbs816Mxs1s9HcMkgAmtNx2M3sS5J+Jel77v6BpE2Slkhaqskj/w+n287dN7v7iLuPzJ077d8DAH3QUdjNbKYmg/5zd/+1JLn7EXc/7+4XJP1E0rLmygRQKht2m/y35dOSDrr7j6ZcP/WUqasl7e99eQB6pZP/xt8laa2k181sX3Xd45LWmNlSSS5pTNJ3Sotps32Wk9p3bkpi6f0qae3lWkylj2lpa65JZ8+erR0rfVxKT2Nd0trrdtnzTv4b/ztJ092zbE8dwODgHXRAEIQdCIKwA0EQdiAIwg4EQdiBIPp6KulSTfZsS3rdpX303FTNJk+5nOv3lp4GO3X7pY9bbrnp1Pa59y6U3u+ckp9Ztz1+juxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EIT1c2laMzsm6f+mXDVf0vG+FfD5DGptg1qXRG3d6mVti939uukG+hr2z+zcbNTdR1orIGFQaxvUuiRq61a/auNpPBAEYQeCaDvsm1vef8qg1jaodUnU1q2+1Nbqa3YA/dP2kR1AnxB2IIhWwm5mK8zsf83sLTN7rI0a6pjZmJm9bmb7zGy05Vq2mNlRM9s/5bohM9thZm9Wl62sqVVT25Nm9sfqsdtnZitbqm2Rmf3WzA6a2QEz+251fauPXaKuvjxufX/NbmYzJL0h6R8kjUvaLWmNu/9PXwupYWZjkkbcvfU3YJjZ1yX9SdLP3P226rp/kXTS3Z+q/lDOdfd/HpDanpT0p7aX8a5WKxqeusy4pPsl/aNafOwSdX1LfXjc2jiyL5P0lrsfcvczkn4haVULdQw8d39Z0smLrl4laWv1+VZN/rL0XU1tA8HdJ9x9b/X5h5I+XWa81ccuUVdftBH2GyT9YcrX4xqs9d5d0m/MbI+ZrW+7mGlc7+4T0uQvj6QFLddzsewy3v100TLjA/PYdbP8eak2wj7dCbQGqf93l7t/TdJ9kh6unq6iMx0t490v0ywzPhC6Xf68VBthH5e0aMrXCyW900Id03L3d6rLo5Ke1eAtRX3k0xV0q8ujLdfzZ4O0jPd0y4xrAB67Npc/byPsuyXdZGZfMbNZkr4taXsLdXyGmV1d/eNEZna1pG9o8Jai3i5pXfX5OknPtVjLXxmUZbzrlhlXy49d68ufu3vfPySt1OR/5N+W9EQbNdTUdaOk/64+DrRdm6Rtmnxad1aTz4gelDRP0k5Jb1aXQwNU239Iel3Sa5oM1nBLtd2tyZeGr0naV32sbPuxS9TVl8eNt8sCQfAOOiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0I4v8BMcSht8xn8hIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step\n",
      "**la clase predicha es:  5\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(imarr, cmap='Greys') #grafica la imagen en colores grises\n",
    "plt.show()\n",
    "\n",
    "imarr_one = np.expand_dims(imarr, 0) #crea un grupo de solo 1 imagen\n",
    "myprediction = model.predict(imarr_one) #predice la imagen\n",
    "print('**la clase predicha es: ', np.argmax(myprediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598147d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sumario\n",
    "- El aprendizaje profundo se aprovecha de las repetidas composiciones de funciones no-lineales para encontrar patrones de estructura jerarquizada en los datos.\n",
    "- Por su naturaleza, el entrenamiento de los modelos requieren de muchos más datos y tiempo de cálculo.\n",
    "- Las CNN destacan por su habilidad en el ámbito del reconocimiento y análisis de imágenes."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
