{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7444f86a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducción al Aprendizaje Profundo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127fc682",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"figures/intro-deep-1.jpg\" width=\"1300\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c24be1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalidades\n",
    "\n",
    "Una ANN realiza **composiciones de funciones** simples para formar una función más compleja.\n",
    "\n",
    "En teoría, una única composición de funciones simples puede aproximar a casi cualquier función compleja, pero esto requiere una **enorme cantidad de parámetros**.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/ann-ex-1.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "Incluso es posible demostrar que un MLP con varias capas ocultas usando la **función identidad**, no tiene ninguna ventaja frente al MLP con solo una capa oculta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d93e3bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El aprendizaje profundo realiza **repetidas composiciones** de funciones no-lineales (muchas capas ocultas), lo que tiene un gran poder expresivo\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{\\mathbf{Y}} = f_1( f_2( f_3( ... f_n( \\mathbf{X} ) ) ) )\n",
    "\\end{align}\n",
    "\n",
    "Esto puede reducir considerablemente la cantidad de parámetros totales necesarios dependiendo de las funciones de activación usadas.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/intro-deep-2.jpg\" width=\"600\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4f21c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las arquitecturas profundas aprovechan mejor los **patrones repetitivos** en los datos para lograr mejor generalización incluso en áreas del espacio con pocos o sin datos.\n",
    "\n",
    "Usualmente estos patrones repetitivos se aprenden como pesos de **atributos jerarquizados**.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/intro-deep-3.gif\" width=\"800\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b124932",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo\n",
    "Una función unidimensional toma alternadamente valores +1 o -1 un total de 8 veces. ¿De qué forma puede construirse una ANN para aproximar esta función?\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/funscale-deep-1.png\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f23b199",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usando una **ANN superficial**, deberían por lo menos haber 8 unidades para aproximar cada valor (sin contar las unidades de sesgo u offset)\n",
    "\n",
    "Así, dependiendo de la entrada, debería \"activarse\" una de las 8 unidades para entregar la salida correcta\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/ann-ex-3.png\" width=\"1000\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9287f581",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usando una **ANN profunda**, se podrían usar 3 capas de 2 unidades para obtener un total de 8 \"caminos\" que se activan para entregar la salida correcta dependiendo de la entrada.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/funscale-deep-3.png\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "Así, la ANN profunda aprende parámetros de forma **jerarquizada**. \n",
    "\n",
    "Por ejemplo, la primera capa aprende un escalón simple, la segunda capa aprende un escalón doble, y sucesivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "602c8c6a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.019870758056640625s\n",
      "Total params:  385\n",
      "0.6853406373795268\n",
      "[ 1.01409946 -1.00355292  0.42308979  0.25430478  0.08908553 -0.07613373\n",
      " -0.24135298 -0.40657224]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "X = [[0.], [1.], [2.], [3.], [4.], [5.], [6.], [7.]]\n",
    "y = [1., -1., 1., -1., 1., -1., 1., -1.]\n",
    "\n",
    "model = MLPRegressor(random_state=4, tol=1.e-8, alpha=1.e-8, hidden_layer_sizes=(128,),\n",
    "                   activation='relu', solver='lbfgs', max_iter=int(1e8), batch_size=1)\n",
    "start = time.time()\n",
    "model.fit(X, y)\n",
    "stop = time.time()\n",
    "\n",
    "print(f\"Training time: {stop - start}s\")\n",
    "total_params = sum(w.size for w in model.coefs_) + sum(b.size for b in model.intercepts_)\n",
    "print(\"Total params: \", total_params)\n",
    "\n",
    "y_predict = model.predict(X)\n",
    "print(mean_squared_error(y, y_predict))\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e56a9916",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.10773944854736328s\n",
      "Total params:  673\n",
      "1.6784778015206738e-09\n",
      "[ 0.99998779 -1.00001646  1.00003037 -1.00001361  1.00000117 -0.99989996\n",
      "  0.99995655 -1.00000155]\n"
     ]
    }
   ],
   "source": [
    "model = MLPRegressor(random_state=4, tol=1.e-8, alpha=1.e-8, hidden_layer_sizes=10*(8, ),\n",
    "                   activation='relu', solver='lbfgs', max_iter=int(1e8), batch_size=1)\n",
    "start = time.time()\n",
    "model.fit(X, y)\n",
    "stop = time.time()\n",
    "\n",
    "print(f\"Training time: {stop - start}s\")\n",
    "total_params = sum(w.size for w in model.coefs_) + sum(b.size for b in model.intercepts_)\n",
    "print(\"Total params: \", total_params)\n",
    "\n",
    "y_predict = model.predict(X)\n",
    "print(mean_squared_error(y, y_predict))\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c365a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Arquitecturas comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e076bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Redes superficiales (shallow network)\n",
    "La mayoría de los modelos de aprendizaje automático (regresión lineal, logística, SVM, PCA, etc.) pueden simularse como una red neuronal de 1 ó 2 capas ocultas.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/ann-5.png\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "Un tipo de red superficial sería el perceptrón multicapa con 1 ó 2 capas ocultas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dd2855",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Estas redes están totalmente conectadas, es decir, hay conexión directa entre todas las unidades.\n",
    "\n",
    "Son un tipo de red prealimentada (feed forward) en las que el flujo va direccionalmente desde capas anteriores a capas posteriores.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/ann-gif-1.gif\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7012729a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Redes de base radial (RBF, radial basis function)\n",
    "\n",
    "A pesar de no ser profunda, difiere de las redes prealimentadas ya que tienen una parte entrenada no supervisadamente y una parte entrenada supervisadamente.\n",
    "\n",
    "Comúnmente tienen una capa oculta con una cantidad de unidades superior al de la capa de entrada.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/rbf-1.png\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f3cd96",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Primero, de manera no supervisada se obtiene el **vector prototipo** $\\bar{\\mu_i}$ para la $i$-ésima unidad oculta. Además, se obtiene un **ancho de banda** $\\sigma_i$ para cada unidad oculta.\n",
    "\n",
    "Luego, para cada vector de atributos $\\bar{X}$ que pasa a la capa oculta se define la **función de activación radial** $\\phi_{i}(\\bar{X})$ como sigue:\n",
    "\n",
    "\\begin{split}\n",
    "    h_{i} = \\phi_{i}(\\bar{X}) = \\text{exp} \\left( {- \\frac{ \\| \\bar{X} - \\bar{\\mu_i} \\|^{2} }{ 2 \\sigma_{i}^2 } } \\right) \\; \\forall i \\in \\{1, ..., m\\}\n",
    "\\end{split}\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/rbf-2.jpeg\" width=\"500\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ebd672",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cada una de las $m$ unidades ocultas tendrá gran influencia en los datos cercanos a su vector prototipo. Es decir, este tipo de red actúa como un método de **agrupamiento**.\n",
    "\n",
    "Luego se calcula la predicción en la capa de salida de la red como se fuera un **perceptrón**, es decir:\n",
    "\n",
    "\\begin{split}\n",
    "    \\hat{y} = \\sum_{i=1}^{m} w_{i} \\phi_{i}(\\bar{X}) = \\sum_{i=1}^{m} w_{i} \\text{exp} \\left( {- \\frac{ \\| \\bar{X} - \\bar{\\mu_i} \\|^{2} }{ 2 \\sigma_{i}^2 } } \\right)\n",
    "\\end{split}\n",
    "\n",
    "Los valores de los pesos $w_{i}$ se aprenden de forma **supervisada** como en una red prealimentada. También se implementa una neurona de sesgo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252aff3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Máquinas restringidas de Boltzmann\n",
    "\n",
    "Utilizan el concepto de minimización de energía para crear una red neuronal de forma no supervisada.\n",
    "\n",
    "También se puede ser un entrenamiento posterior de forma supervisada.\n",
    "\n",
    "Es diferente a las redes prealimentadas en varios sentidos:\n",
    "- Modela la **probabilidad conjunta** de los atributos en vez de minimizar una función de costo.\n",
    "- Tienen **flujo no directo**, ya que aprenden relaciones probabilísticas en vez de mapeos entrada-salida.\n",
    "- Crea representaciones **latentes** (ocultas) de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c036648f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Redes neuronales recurrentes\n",
    "\n",
    "Las conexiones entre unidades pueden crear **ciclos**, por lo que la salida de una neurona puede afectar a su propia entrada.\n",
    "\n",
    "Son especialmente útiles para modelar comportamiento **temporales** o dinámicos.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/recurrent-1.png\" width=\"500\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066a000",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Redes neuronales convolucionales \n",
    "\n",
    "Históricamente han sido el tipo de ANN más exitosa, usadas principalmente para reconocimiento de imágenes, localización de objetos y procesamiento de texto, entre otros.\n",
    "\n",
    "Están inspiradas por el funcionamiento del cortex visual de los gatos, en donde porciones específicas del campo visual activan ciertas neuronas.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/convolution-2-gif.gif\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df6bab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Redes generativas adversativas\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/GAN.png\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778dca54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementación en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dd9af6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Librerías\n",
    "\n",
    "Hay principalmente 3 librerías para implementar modelos de aprendizaje profundo en Python:\n",
    "\n",
    "- Tensorflow\n",
    "- Pytorch\n",
    "- Keras\n",
    "\n",
    "Estas librerías permiten formar fácilmente capas convolucionales, pooling, etc.\n",
    "\n",
    "Se diferencian en cuanto a la complejidad de usar, compatibilidad, velocidad, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9bf651",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"figures/comparison-deep.png\" width=\"1200\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cf765",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uso de la GPU\n",
    "\n",
    "Debido a la considerable mayor cantidad de atributos y operaciones computacionales presentes en una red profunda, los tiempos de entrenamiento son mucho mayores.\n",
    "\n",
    "Esto se logra solucionar mediante el uso de la tarjeta gráfica (GPU) en vez del procesador central (CPU) para realizar los cálculos.\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/gpu-1.png\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76a06d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El poder usar la GPU para los cálculos se logra mediante la plataforma **CUDA** de Nvidia.\n",
    "\n",
    "Las librerías anteriores pueden detectar si la GPU del PC es compatible con CUDA. \n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/cuda.jpg\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "En **Keras**, si la GPU es compatible los cálculos se realizarán automáticamente en ella.\n",
    "\n",
    "Aquí se puede revisar el listado de GPUs compatibles:\n",
    "\n",
    "https://developer.nvidia.com/cuda-gpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d90572",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Links para configuración de TensorFlow\n",
    "\n",
    "https://www.tensorflow.org/install/pip?hl=es-419#windows-wsl2_1\n",
    "\n",
    "https://www.tensorflow.org/install/gpu?hl=es-419\n",
    "\n",
    "https://www.youtube.com/watch?v=0S81koZpwPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75900d3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ejemplo de MLP en Keras (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f8049e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de atributos =  4\n",
      "Número de clases =  3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "print('Número de atributos = ', num_features)\n",
    "print('Número de clases = ', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "15b4e4c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Normalización estándar\n",
    "normalizer = layers.Normalization()\n",
    "normalizer.adapt(X_train)\n",
    "\n",
    "#Construye modelo\n",
    "model = keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(8, activation=\"relu\"),\n",
    "    layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "#Definición de hiperparámetros del entrenamiento (Compilación)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d2bf0a0e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9667 - loss: 0.0486 - val_accuracy: 1.0000 - val_loss: 0.0092\n",
      "Epoch 2/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9719 - loss: 0.0501 - val_accuracy: 1.0000 - val_loss: 0.0094\n",
      "Epoch 3/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9737 - loss: 0.0502 - val_accuracy: 1.0000 - val_loss: 0.0093\n",
      "Epoch 4/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9808 - loss: 0.0319 - val_accuracy: 1.0000 - val_loss: 0.0089\n",
      "Epoch 5/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9696 - loss: 0.0544 - val_accuracy: 1.0000 - val_loss: 0.0089\n",
      "Epoch 6/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9719 - loss: 0.0442 - val_accuracy: 1.0000 - val_loss: 0.0087\n",
      "Epoch 7/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9826 - loss: 0.0367 - val_accuracy: 1.0000 - val_loss: 0.0089\n",
      "Epoch 8/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9853 - loss: 0.0386 - val_accuracy: 1.0000 - val_loss: 0.0088\n",
      "Epoch 9/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9853 - loss: 0.0287 - val_accuracy: 1.0000 - val_loss: 0.0088\n",
      "Epoch 10/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9533 - loss: 0.0653 - val_accuracy: 1.0000 - val_loss: 0.0088\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "351eede5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.967  |  Test loss: 0.0758\n"
     ]
    }
   ],
   "source": [
    "#Evaluacion\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {acc:.3f}  |  Test loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
